# Выводы по заданию 5ого дня

1. Температура близкая к нулю выдает маловариативные ответы. Полезно при решении математических задач и при написании алгоритмов чего-либо (написание инструкций агентам, чтоб они им следовали не отходя от них). Пример: JSON-форматирование или XML-форматирование 2ого дня челленджа.

2. Температура близкая к середине полезна при решении инженерных задач, где немалую роль играет иногда нестандартный подход но в рамках и правилах какой-то системы координат (физический мир, язык программирования, использование определенных ингредиентов). Задача которая передается на решение агенту может являеться умеренно понятной человеку. Пример: форматирование текста в деловой стиль, человек точно знает что там не должно быть мата, но не знает наверняка в чем заключается деловой стиль письма.

3. Температура близка к 1 имеет высоковариативные ответы и подходит для решения творческих задач или задач, в которых человек слабо разбирается или не разбирается вообще. При высокой температуре модель не нуждается в готовой структуре, поэтому может сгенерировать направление для исследования даже там, где у человека нет экспертности. Такие задачи, как правило, почти не ограничены какой-то системой координат или инструкциями. Примеры таких задач: написание стихов, составление рецептов, написание MVP какого-то проекта.

# Вывод по заданию 6ого дня

Разные модели по-разному обучены на генерацию результатов. Генерация может отличаться как по времени так и по количеству токенов. 
В качестве сравнения были выбраны 3 модели: Sao10K/L3-8B-Stheno-v3.2, MiniMaxAI/MiniMax-M2:novita и Qwen/Qwen2.5-7B-Instruct. 
Qwen/Qwen2.5-7B-Instruct значительно лучшие результаты показала как по количеству токенов так и по времени запроса, однако обширность знаний показала лучше MiniMaxAI/MiniMax-M2:novita. 
Таким образом, можно сделать вывод что всегда будет что-то проседать и все зависит от LLM и целей которые перед ней ставятся. 
Треугольник: быстродействие, стоимость и качество ответа, всегда будет работать 2 из 3.

# Вывод по заданию 7ого дня

У каждой LLM есть контекстное окно - это суммарное количество токенов от пользователя и от LLM. Контекстное окно - это буквально "память" LLM и в интересах пользователя не выходить за пределы этого окна, в противном случае долгий диалог теряет связь и, как говорится, LLM начинает  "галлюцинировать". Существует множество способов не выйти за рамки окна, но вместить больше полезной информации, такие как, например, суммаризация. Со стороны LLM также можно сделать ограничение на максимальное количество токенов, которое она может потратить на ответ пользователя, но это напрямую влияет на качество и полноту ответа и на потерю важной информации.

# День 8. Сжатие диалога

Суммаризация - очень классный инструмент общения с LLM, когда нужно сэкономить существенную долю токенов, но при этом не упустить суть диалога. Однако суммаризация диалога может привести к компрессии действительно важных для диалога вещей (например обращение, по которому LLM попросили общаться). Это могут быть более чувствительные данные поэтому компрессию нужно также очень внимательно описывать, чтоб не упустить важные детали (эти важные детали нужно описать отдельно). 
